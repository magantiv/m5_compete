{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M5 Forecasting - Accuracy TimeSeriesSplit CV- Custom Loss\n",
    "\n",
    "Xiao Song\n",
    "\n",
    "<https://xsong.ltd/en>     \n",
    "[Kaggle profile](https://www.kaggle.com/rikdifos/)\n",
    "\n",
    "The competition webpage: <https://www.kaggle.com/c/m5-forecasting-accuracy>\n",
    "\n",
    "This notebook is mainly based on: [Simple LGBM GroupKFold CV](https://www.kaggle.com/ragnar123/simple-lgbm-groupkfold-cv/), shout out to the author [ragnar](https://www.kaggle.com/ragnar123/).\n",
    "\n",
    "\n",
    "\n",
    "`sales_train_validation` contains d_1-d_1913 (2011-01-29 ~ 2011-04-24) sales data of 30490 rows.   \n",
    "`sales_train_evaluation`  contains d_1-d_1941 (2011-01-29 ~ 2011-05-22) sales data of 30490 rows.      \n",
    "`submission` has 60980 rows, public LB calulates `sales_train_validation`(d_1914-d_1941)  \n",
    "Private LB evaluations `sales_train_evaluation`(d_1942-d_1968).\n",
    "\n",
    "This notebook use a self-defined loss function from [this notebook](https://www.kaggle.com/ragnar123/simple-lgbm-groupkfold-cv).\n",
    "\n",
    "This notebook will output a 'submission3.csv' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import lightgbm as lgb\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn import preprocessing, metrics\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, TimeSeriesSplit\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    '''reduce RAM usage\n",
    "    '''\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def read_data():\n",
    "    '''data input\n",
    "    '''\n",
    "    print('Reading files...')\n",
    "    calendar = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\n",
    "    calendar = reduce_mem_usage(calendar)\n",
    "    print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n",
    "    sell_prices = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\n",
    "    sell_prices = reduce_mem_usage(sell_prices)\n",
    "    print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n",
    "    sales_train_evaluation = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv')\n",
    "    print('Sales train evaluation has {} rows and {} columns'.format(sales_train_evaluation.shape[0], sales_train_evaluation.shape[1]))\n",
    "    submission = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')\n",
    "    return calendar, sell_prices, sales_train_evaluation, submission\n",
    "\n",
    "\n",
    "def melt_and_merge(calendar, sell_prices, sales_train_evaluation, submission, nrows = 55000000, merge = False):\n",
    "    \n",
    "    # melt sales data, get it ready for training\n",
    "    sales_train_evaluation = pd.melt(sales_train_evaluation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n",
    "    print('Melted sales train validation has {} rows and {} columns'.format(sales_train_evaluation.shape[0], sales_train_evaluation.shape[1]))\n",
    "    sales_train_evaluation = reduce_mem_usage(sales_train_evaluation)\n",
    "    \n",
    "    # seperate test dataframes\n",
    "    test1_rows = [row for row in submission['id'] if 'validation' in row]\n",
    "    test2_rows = [row for row in submission['id'] if 'evaluation' in row]\n",
    "    test1 = submission[submission['id'].isin(test1_rows)]\n",
    "    test2 = submission[submission['id'].isin(test2_rows)]\n",
    "    \n",
    "    # change column names\n",
    "    test1.columns = ['id', 'd_1914', 'd_1915', 'd_1916', 'd_1917', 'd_1918', 'd_1919', 'd_1920', 'd_1921', 'd_1922', 'd_1923', 'd_1924', 'd_1925', 'd_1926', 'd_1927', 'd_1928', 'd_1929', 'd_1930', 'd_1931', \n",
    "                      'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937', 'd_1938', 'd_1939', 'd_1940', 'd_1941']\n",
    "    test2.columns = ['id', 'd_1942', 'd_1943', 'd_1944', 'd_1945', 'd_1946', 'd_1947', 'd_1948', 'd_1949', 'd_1950', 'd_1951', 'd_1952', 'd_1953', 'd_1954', 'd_1955', 'd_1956', 'd_1957', 'd_1958', 'd_1959', \n",
    "                      'd_1960', 'd_1961', 'd_1962', 'd_1963', 'd_1964', 'd_1965', 'd_1966', 'd_1967', 'd_1968', 'd_1969']\n",
    "    \n",
    "    # get product table\n",
    "    product = sales_train_evaluation[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n",
    "    \n",
    "    # merge with product table\n",
    "    product['id'] = product['id'].str.replace('_evaluation','_validation')\n",
    "    test1 = test1.merge(product, how = 'left', on = 'id') # validation\n",
    "    product['id'] = product['id'].str.replace('_validation','_evaluation')\n",
    "    test2 = test2.merge(product, how = 'left', on = 'id') # evaluation\n",
    "    \n",
    "    # \n",
    "    test1 = pd.melt(test1, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n",
    "    test2 = pd.melt(test2, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n",
    "    \n",
    "    sales_train_evaluation['part'] = 'train'\n",
    "    test1['part'] = 'validation'\n",
    "    test2['part'] = 'evaluation'\n",
    "    \n",
    "    data = pd.concat([sales_train_evaluation, test1, test2], axis = 0)\n",
    "    \n",
    "    del sales_train_evaluation, test1, test2\n",
    "    \n",
    "    # get only a sample for fst training\n",
    "    data = data.loc[nrows:]\n",
    "    \n",
    "    # drop some calendar features\n",
    "    # calendar.drop(['weekday', 'wday', 'month', 'year','snap_CA','snap_TX','snap_WI'], inplace = True, axis = 1)\n",
    "    \n",
    "    # delete test2 for now\n",
    "    data = data[data['part'] != 'validation']\n",
    "    \n",
    "    if merge:\n",
    "        # notebook crash with the entire dataset (maybee use tensorflow, dask, pyspark xD)\n",
    "        data = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\n",
    "        data.drop(['weekday', 'wday', 'month', 'year','snap_CA','snap_TX','snap_WI'], inplace = True, axis = 1)\n",
    "        # get the sell price data (this feature should be very important)\n",
    "        data = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n",
    "        print('Our final dataset to train has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n",
    "    else: \n",
    "        pass\n",
    "    \n",
    "    # data.to_pickle('data_clean.pkl')\n",
    "    gc.collect()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "\n",
    "def transform(data):\n",
    "    '''data transformation\n",
    "    '''\n",
    "    start = time.time()\n",
    "    nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in nan_features:\n",
    "        data[feature].fillna('unknown', inplace = True)\n",
    "    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in cat:\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        data[feature] = encoder.fit_transform(data[feature])\n",
    "    print('Data transformation costs %7.2f seconds'%(time.time()-start))\n",
    "    return data\n",
    "\n",
    "def simple_fe(data):\n",
    "    '''do some feature engineering\n",
    "    '''\n",
    "    start = time.time()\n",
    "    # rolling demand features\n",
    "    data_fe = data[['id', 'demand']]\n",
    "    \n",
    "    window = 28\n",
    "    periods = [7, 15, 30, 90]\n",
    "    group = data_fe.groupby('id')['demand']\n",
    "    \n",
    "    # most recent lag data\n",
    "    for period in periods:\n",
    "        data_fe['demand_rolling_mean_t' + str(period)] = group.transform(lambda x: x.shift(window).rolling(period).mean())\n",
    "\n",
    "    periods = [7, 90]\n",
    "    for period in periods:\n",
    "        data_fe['demand_rolling_std_t' + str(period)] = group.transform(lambda x: x.shift(window).rolling(period).std())\n",
    "        \n",
    "    # reduce memory\n",
    "    data_fe = reduce_mem_usage(data_fe)\n",
    "    \n",
    "    # get time features\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    time_features = ['year', 'month', 'week', 'day', 'dayofweek', 'dayofyear']\n",
    "    dtype = np.int16\n",
    "    for time_feature in time_features:\n",
    "        data[time_feature] = getattr(data['date'].dt, time_feature).astype(dtype)\n",
    "        \n",
    "    # concat lag and rolling features with main table\n",
    "    lag_rolling_features = [col for col in data_fe.columns if col not in ['id', 'demand']]\n",
    "    data = pd.concat([data, data_fe[lag_rolling_features]], axis = 1)\n",
    "    data['weekends'] = np.where((data['date'].dt.dayofweek) < 5, 0, 1) # generate weekends\n",
    "\n",
    "    del data_fe\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Simple feature engineering costs %7.2f seconds'%(time.time()-start))\n",
    "    return data\n",
    "\n",
    "def custom_asymmetric_train(y_pred, y_true):\n",
    "    '''define custom loss function\n",
    "    '''\n",
    "    y_true = y_true.get_label()\n",
    "    residual = (y_true - y_pred).astype(\"float\")\n",
    "    grad = np.where(residual < 0, -2 * residual, -2 * residual * 1.15)\n",
    "    hess = np.where(residual < 0, 2, 2 * 1.15)\n",
    "    return grad, hess\n",
    "\n",
    "def run_lgb(data):\n",
    "    '''cross validation\n",
    "    '''\n",
    "    start = time.time()\n",
    "    \n",
    "    data = data.sort_values('date')\n",
    "    \n",
    "    x_train = data[data['date'] <= '2016-05-22']\n",
    "    y_train = x_train['demand']\n",
    "    \n",
    "    test = data[(data['date'] > '2016-05-22')]\n",
    "    \n",
    "\n",
    "    del data\n",
    "    gc.collect()\n",
    "\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'metric': 'rmse',\n",
    "        'seed': 225,\n",
    "        'learning_rate': 0.02,\n",
    "        'lambda': 0.4, # l2 regularization\n",
    "        'reg_alpha': 0.4, # l1 regularization\n",
    "        'max_depth': 5, # max depth of decision trees\n",
    "        'num_leaves': 64, # number of leaves\n",
    "        'bagging_fraction': 0.7, # bootstrap sampling\n",
    "        'bagging_freq' : 1,\n",
    "        'colsample_bytree': 0.7 # feature sampling\n",
    "    }\n",
    "    \n",
    "    oof = np.zeros(len(x_train))\n",
    "    preds = np.zeros(len(test))\n",
    "    \n",
    "    n_fold = 3 #3 for timely purpose of the kernel\n",
    "    folds = TimeSeriesSplit(n_splits=n_fold)\n",
    "    splits = folds.split(x_train, y_train)\n",
    "\n",
    "\n",
    "    #feature_importances = pd.DataFrame()\n",
    "    #feature_importances['feature'] = features\n",
    "    \n",
    "    for fold, (trn_idx, val_idx) in enumerate(splits):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        \n",
    "        train_set = lgb.Dataset(x_train.iloc[trn_idx][features], y_train.iloc[trn_idx], categorical_feature = cat)\n",
    "        \n",
    "        val_set = lgb.Dataset(x_train.iloc[val_idx][features], y_train.iloc[val_idx], categorical_feature = cat)\n",
    "\n",
    "        model = lgb.train(params, train_set, num_boost_round = 2400, early_stopping_rounds = 50, \n",
    "                          valid_sets = [val_set], verbose_eval = 50, fobj = custom_asymmetric_train)\n",
    "        \n",
    "        \n",
    "        #lgb.plot_importance(model, importance_type = 'gain', precision = 0,\n",
    "         #                       height = 0.5, figsize = (6, 10), title = '') \n",
    "        \n",
    "        #feature_importances[f'fold_{fold + 1}'] = model.feature_importance()\n",
    "        oof[val_idx] = model.predict(x_train.iloc[val_idx][features]) # cv prediction\n",
    "        preds += model.predict(test[features]) / 3 # calculate mean prediction value of 3 models\n",
    "        print('-' * 50)\n",
    "        print('\\n')\n",
    "    #model.save_model('model.lgb')\n",
    "    del x_train\n",
    "        \n",
    "    print('3 folds cross-validation costs %7.2f seconds'%(time.time() - start))\n",
    "\n",
    "    oof_rmse = np.sqrt(metrics.mean_squared_error(y_train, oof))\n",
    "    print(f'Our out of folds rmse is {oof_rmse}')\n",
    "    del y_train\n",
    "        \n",
    "    test = test[['id', 'date', 'demand']]\n",
    "    test['demand'] = preds\n",
    "    gc.collect()\n",
    "    return test \n",
    "\n",
    "\n",
    "\n",
    "def predict(test, submission):\n",
    "    '''predict test and validation data label\n",
    "    '''\n",
    "    start = time.time()\n",
    "    predictions = test[['id', 'date', 'demand']]\n",
    "    predictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'demand').reset_index()\n",
    "    predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "    #predictions.to_csv('predictions.csv', index = False)\n",
    "\n",
    "    prediction_val = predictions.copy()\n",
    "    prediction_val['id'] = prediction_val['id'].str.replace('_evaluation','_validation') # change id to validation\n",
    "    #prediction_val.to_csv('prediction_val.csv', index = False)\n",
    "    \n",
    "    concated = pd.concat([predictions, prediction_val])\n",
    "    del predictions, prediction_val\n",
    "    print('final dataset to train has {} rows and {} columns'.format(concated.shape[0], concated.shape[1]))\n",
    "    concated.to_csv('submission3.csv', index = False)\n",
    "    print('Data prediction costs %7.2f seconds'%(time.time() - start))\n",
    "    \n",
    "\n",
    "# define list of features\n",
    "features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'sell_price', 'year', \n",
    "                'month', 'week', 'day', 'dayofweek', 'dayofyear', 'demand_rolling_mean_t7', 'demand_rolling_mean_t15', 'demand_rolling_mean_t30', 'demand_rolling_mean_t90',\n",
    "                'demand_rolling_std_t7', 'demand_rolling_std_t90','weekends']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(): \n",
    "    '''run the all program\n",
    "    '''\n",
    "    calendar, sell_prices, sales_train_evaluation, submission = read_data()\n",
    "    data = melt_and_merge(calendar, sell_prices, sales_train_evaluation, submission, nrows = 27500000, merge = True)\n",
    "    data = transform(data)\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    days = abs((data['date'].min() - data['date'].max()).days)\n",
    "    # how many training data do we need to train with at least 2 years and consider lags\n",
    "    need = 365 + 365 + 90 + 28\n",
    "    print(f'We have {(days - 28)} days of training history')\n",
    "    print(f'we have {(days - 28 - need)} days left')\n",
    "    if (days - 28 - need) > 0:\n",
    "        print('We have enought training data, lets continue')\n",
    "    else:\n",
    "        print('Get more training data, training can fail')\n",
    "        \n",
    "    data = simple_fe(data)\n",
    "    # reduce memory for new features so we can train\n",
    "    data = reduce_mem_usage(data)\n",
    "\n",
    "    print('Removing first 118 days')\n",
    "    # eliminate the first 118 days of our train data because of lags\n",
    "    min_date = data['date'].min() + timedelta(days = 118)\n",
    "    data = data[data['date'] > min_date]\n",
    "    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "\n",
    "    test = run_lgb(data)\n",
    "    del data\n",
    "    predict(test, submission) \n",
    "    #feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(3)]].mean(axis=1)\n",
    "    #feature_importances.to_csv('feature_importances.csv')\n",
    "\n",
    "    #plt.figure(figsize=(16, 12))\n",
    "    #sns.barplot(data=feature_importances.sort_values(by='average', ascending = False).head(20), x='average', y='feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_and_evaluate() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Reading files...\n",
    "Mem. usage decreased to  0.12 Mb (41.9% reduction)\n",
    "Calendar has 1969 rows and 14 columns\n",
    "Mem. usage decreased to 130.48 Mb (37.5% reduction)\n",
    "Sell prices has 6841121 rows and 4 columns\n",
    "Sales train evaluation has 30490 rows and 1947 columns\n",
    "Melted sales train validation has 59181090 rows and 8 columns\n",
    "Mem. usage decreased to 3273.49 Mb (9.4% reduction)\n",
    "Our final dataset to train has 32534810 rows and 17 columns\n",
    "Data transformation costs  138.08 seconds\n",
    "We have 1039 days of training history\n",
    "we have 191 days left\n",
    "We have enought training data, lets continue\n",
    "Mem. usage decreased to 930.83 Mb (58.3% reduction)\n",
    "Simple feature engineering costs  334.56 seconds\n",
    "Mem. usage decreased to 2389.13 Mb (50.6% reduction)\n",
    "Removing first 118 days\n",
    "Training fold 1\n",
    "Training until validation scores don't improve for 50 rounds\n",
    "[50]\tvalid_0's rmse: 2.64232\n",
    "[100]\tvalid_0's rmse: 2.42706\n",
    "[150]\tvalid_0's rmse: 2.39864\n",
    "[200]\tvalid_0's rmse: 2.39592\n",
    "[250]\tvalid_0's rmse: 2.39744\n",
    "Early stopping, best iteration is:\n",
    "[211]\tvalid_0's rmse: 2.39555\n",
    "--------------------------------------------------\n",
    "\n",
    "\n",
    "Training fold 2\n",
    "Training until validation scores don't improve for 50 rounds\n",
    "[50]\tvalid_0's rmse: 2.62458\n",
    "[100]\tvalid_0's rmse: 2.38028\n",
    "[150]\tvalid_0's rmse: 2.33064\n",
    "[200]\tvalid_0's rmse: 2.31464\n",
    "[250]\tvalid_0's rmse: 2.30482\n",
    "[300]\tvalid_0's rmse: 2.29913\n",
    "[350]\tvalid_0's rmse: 2.29549\n",
    "[400]\tvalid_0's rmse: 2.29169\n",
    "[450]\tvalid_0's rmse: 2.28907\n",
    "[500]\tvalid_0's rmse: 2.28664\n",
    "[550]\tvalid_0's rmse: 2.28493\n",
    "[600]\tvalid_0's rmse: 2.28329\n",
    "[650]\tvalid_0's rmse: 2.28163\n",
    "[700]\tvalid_0's rmse: 2.28048\n",
    "[750]\tvalid_0's rmse: 2.27963\n",
    "[800]\tvalid_0's rmse: 2.27861\n",
    "[850]\tvalid_0's rmse: 2.27778\n",
    "[900]\tvalid_0's rmse: 2.27684\n",
    "[950]\tvalid_0's rmse: 2.27585\n",
    "[1000]\tvalid_0's rmse: 2.27522\n",
    "[1050]\tvalid_0's rmse: 2.27422\n",
    "[1100]\tvalid_0's rmse: 2.27357\n",
    "[1150]\tvalid_0's rmse: 2.27285\n",
    "[1200]\tvalid_0's rmse: 2.2722\n",
    "[1250]\tvalid_0's rmse: 2.27182\n",
    "[1300]\tvalid_0's rmse: 2.27125\n",
    "[1350]\tvalid_0's rmse: 2.27082\n",
    "[1400]\tvalid_0's rmse: 2.2703\n",
    "[1450]\tvalid_0's rmse: 2.27018\n",
    "[1500]\tvalid_0's rmse: 2.26987\n",
    "[1550]\tvalid_0's rmse: 2.26946\n",
    "[1600]\tvalid_0's rmse: 2.26929\n",
    "Early stopping, best iteration is:\n",
    "[1593]\tvalid_0's rmse: 2.26914\n",
    "--------------------------------------------------\n",
    "\n",
    "\n",
    "Training fold 3\n",
    "Training until validation scores don't improve for 50 rounds\n",
    "[50]\tvalid_0's rmse: 2.56816\n",
    "[100]\tvalid_0's rmse: 2.34487\n",
    "[150]\tvalid_0's rmse: 2.30617\n",
    "[200]\tvalid_0's rmse: 2.29267\n",
    "[250]\tvalid_0's rmse: 2.28519\n",
    "[300]\tvalid_0's rmse: 2.2817\n",
    "[350]\tvalid_0's rmse: 2.27822\n",
    "[400]\tvalid_0's rmse: 2.27663\n",
    "[450]\tvalid_0's rmse: 2.27589\n",
    "[500]\tvalid_0's rmse: 2.27493\n",
    "Early stopping, best iteration is:\n",
    "[498]\tvalid_0's rmse: 2.27492\n",
    "--------------------------------------------------\n",
    "\n",
    "\n",
    "3 folds cross-validation costs 5652.74 seconds\n",
    "Our out of folds rmse is 2.8134522812366525\n",
    "final dataset to train has 91470 rows and 29 columns\n",
    "Data prediction costs   10.57 seconds\n",
    "final dataset to train has 91470 rows and 29 columns\n",
    "Data prediction costs   10.61 seconds\n",
    "CPU times: user 5h 20min 35s, sys: 13min 14s, total: 5h 33min 49s\n",
    "Wall time: 1h 48min 10s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
