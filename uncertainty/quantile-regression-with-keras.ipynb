{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "code, kbd, pre, samp {\n",
    "    font-family:'consolas', Lucida Console, SimSun, Fira Code, Monaco !important;\n",
    "    font-size: 11pt !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantile Regression with Keras  \n",
    "\n",
    "Xiao Song\n",
    "\n",
    "<https://xsong.ltd/en>     \n",
    "[Kaggle profile](https://www.kaggle.com/rikdifos/)\n",
    "\n",
    "The competition webpage: <https://www.kaggle.com/c/m5-forecasting-uncertainty>\n",
    "\n",
    "+ This notebook is a [public kernel](https://www.kaggle.com/ulrich07/quantile-regression-with-keras) released by [Ulrich GOUE](https://www.kaggle.com/ulrich07), thanks to the author!  \n",
    "\n",
    "+ This notebook serves as one single model of the final mean-based-blending.  \n",
    "\n",
    "+ This notebook will output a './submission_from_keras.csv' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "#\n",
    "def autocorrelation(ys, t=1):\n",
    "    return np.corrcoef(ys[:-t], ys[t:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================================================================\n",
    "def preprocess_sales(sales, start=1400, upper=1970):\n",
    "    '''process sales data\n",
    "    '''\n",
    "    if start is not None:\n",
    "        print(\"dropping...\")\n",
    "        to_drop = [f\"d_{i+1}\" for i in range(start-1)]\n",
    "        print(sales.shape)\n",
    "        sales.drop(to_drop, axis=1, inplace=True)\n",
    "        print(sales.shape)\n",
    "    #=======\n",
    "    print(\"adding...\")\n",
    "    new_columns = ['d_%i'%i for i in range(1942, upper, 1)] # 1942-1970 \n",
    "    for col in new_columns:\n",
    "        sales[col] = np.nan\n",
    "    print(\"melting...\")\n",
    "    sales = sales.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\",\"scale\",\"start\"],\n",
    "                        var_name='d', value_name='demand')\n",
    "\n",
    "    print(\"generating order\")\n",
    "    if start is not None:\n",
    "        skip = start\n",
    "    else:\n",
    "        skip = 1\n",
    "    sales[\"nb\"] = sales.index // 42840 + skip\n",
    "    return sales\n",
    "#===============================================================\n",
    "def preprocess_calendar(calendar):\n",
    "    '''clean and transform calendar data\n",
    "    '''\n",
    "    global maps, mods\n",
    "    calendar[\"event_name\"] = calendar[\"event_name_1\"]\n",
    "    calendar[\"event_type\"] = calendar[\"event_type_1\"]\n",
    "\n",
    "    map1 = {mod:i for i,mod in enumerate(calendar['event_name'].unique())}\n",
    "    calendar['event_name'] = calendar['event_name'].map(map1)\n",
    "    map2 = {mod:i for i,mod in enumerate(calendar['event_type'].unique())}\n",
    "    calendar['event_type'] = calendar['event_type'].map(map2)\n",
    "    calendar['nday'] = calendar['date'].str[-2:].astype(int)\n",
    "    maps[\"event_name\"] = map1\n",
    "    maps[\"event_type\"] = map2\n",
    "    mods[\"event_name\"] = len(map1)\n",
    "    mods[\"event_type\"] = len(map2)\n",
    "    calendar[\"wday\"] -=1\n",
    "    calendar[\"month\"] -=1\n",
    "    calendar[\"year\"] -= 2011\n",
    "    mods[\"month\"] = 12\n",
    "    mods[\"year\"] = 6\n",
    "    mods[\"wday\"] = 7\n",
    "    mods['snap_CA'] = 2\n",
    "    mods['snap_TX'] = 2\n",
    "    mods['snap_WI'] = 2\n",
    "\n",
    "    calendar.drop([\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\", \"date\", \"weekday\"], \n",
    "                  axis=1, inplace=True)\n",
    "    return calendar\n",
    "#=========================================================\n",
    "def make_dataset(categorize=False ,start=1400, upper= 1970):\n",
    "    global maps, mods\n",
    "    print(\"loading calendar...\")\n",
    "    calendar = pd.read_csv(\"../input/m5-forecasting-uncertainty/calendar.csv\")\n",
    "    print(\"loading sales...\")\n",
    "    sales = pd.read_csv(\"../input/walmartadd/sales.csv\")\n",
    "    cols = [\"item_id\", \"dept_id\", \"cat_id\",\"store_id\",\"state_id\"]\n",
    "    if categorize:\n",
    "        for col in cols:\n",
    "            temp_dct = {mod:i for i, mod in enumerate(sales[col].unique())}\n",
    "            mods[col] = len(temp_dct)\n",
    "            maps[col] = temp_dct\n",
    "        for col in cols:\n",
    "            sales[col] = sales[col].map(maps[col])\n",
    "        #\n",
    "\n",
    "    sales =preprocess_sales(sales, start=start, upper= upper)\n",
    "    calendar = preprocess_calendar(calendar)\n",
    "    calendar = reduce_mem_usage(calendar)\n",
    "    print(\"merge with calendar...\")\n",
    "    sales = sales.merge(calendar, on='d', how='left')\n",
    "    del calendar\n",
    "\n",
    "    print(\"reordering...\")\n",
    "    sales.sort_values(by=[\"id\",\"nb\"], inplace=True)\n",
    "    print(\"re-indexing..\")\n",
    "    sales.reset_index(inplace=True, drop=True)\n",
    "    gc.collect()\n",
    "\n",
    "    sales['n_week'] = (sales['nb']-1)//7\n",
    "    sales[\"nday\"] -= 1\n",
    "    mods['nday'] = 31\n",
    "    sales = reduce_mem_usage(sales)\n",
    "    gc.collect()\n",
    "    return sales\n",
    "#===============================================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "CATEGORIZE = True;\n",
    "START = 1400; UPPER = 1970;\n",
    "maps = {}\n",
    "mods = {}\n",
    "sales = make_dataset(categorize=CATEGORIZE ,start=START, upper= UPPER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales[\"x\"] = sales[\"demand\"] / sales[\"scale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAGS = [28, 35, 42, 49, 56, 63]\n",
    "FEATS = []\n",
    "for lag in tqdm(LAGS):\n",
    "    sales[f\"x_{lag}\"] = sales.groupby(\"id\")[\"x\"].shift(lag)\n",
    "    FEATS.append(f\"x_{lag}\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sales.loc[(sales.start>1844)&(sales.nb>1840)&(sales.nb<1850), ['id','start','nb','demand']]\n",
    "#sales.start.max() #1845"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sales.shape)\n",
    "sales = sales.loc[sales.nb>sales.start]\n",
    "print(sales.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = sales['nb'].values\n",
    "MAX_LAG = max(LAGS)\n",
    "#tr_mask = np.logical_and(nb>START + MAX_LAG, nb<=1913)\n",
    "tr_mask = np.logical_and(nb>START + MAX_LAG, nb<=1941) # SORRY THIS IS FAKE VALIDATION. I DIDN'T THINK IT WOULD HAVE HAD LIFTED UP MY SCORE LIKE THAT\n",
    "val_mask = np.logical_and(nb>1913, nb<=1941)\n",
    "te_mask = np.logical_and(nb>1941, nb<=1969)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = sales['scale'].values\n",
    "ids = sales['id'].values\n",
    "#y = sales['demand'].values\n",
    "#ys = y / scale\n",
    "ys = sales['x'].values\n",
    "Z = sales[FEATS].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = scale[val_mask]\n",
    "se = scale[te_mask]\n",
    "ids = ids[te_mask]\n",
    "ids = ids.reshape((-1, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca = sales[['snap_CA']].values\n",
    "tx = sales[['snap_TX']].values\n",
    "wi = sales[['snap_WI']].values\n",
    "wday = sales[['wday']].values\n",
    "month = sales[['month']].values\n",
    "year = sales[['year']].values\n",
    "event = sales[['event_name']].values\n",
    "nday = sales[['nday']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = sales[['item_id']].values\n",
    "dept = sales[['dept_id']].values\n",
    "cat = sales[['cat_id']].values\n",
    "store = sales[['store_id']].values\n",
    "state = sales[['state_id']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(mask):\n",
    "    x = {\"snap_CA\":ca[mask], \"snap_TX\":tx[mask], \"snap_WI\":wi[mask], \"wday\":wday[mask], \n",
    "         \"month\":month[mask], \"year\":year[mask], \"event\":event[mask], \"nday\":nday[mask], \n",
    "         \"item\":item[mask], \"dept\":dept[mask], \"cat\":cat[mask], \"store\":store[mask], \n",
    "         \"state\":state[mask], \"num\":Z[mask]}\n",
    "    t = ys[mask]\n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt, yt = make_data(tr_mask) #train\n",
    "xv, yv = make_data(val_mask) # val\n",
    "xe, ye = make_data(te_mask) # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It is a baseline model. Feel free to add your own FE magic !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#=====\n",
    "def qloss(y_true, y_pred):\n",
    "    # Pinball loss for multiple quantiles\n",
    "    qs = [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995]\n",
    "    q = tf.constant(np.array([qs]), dtype=tf.float32)\n",
    "    e = y_true - y_pred\n",
    "    v = tf.maximum(q*e, (q-1)*e)\n",
    "    return K.mean(v)\n",
    "#============================#\n",
    "def make_model(n_in):\n",
    "    \n",
    "    num = L.Input((n_in,), name=\"num\")\n",
    "    \n",
    "    ca = L.Input((1,), name=\"snap_CA\")\n",
    "    tx = L.Input((1,), name=\"snap_TX\")\n",
    "    wi = L.Input((1,), name=\"snap_WI\")\n",
    "    wday = L.Input((1,), name=\"wday\")\n",
    "    month = L.Input((1,), name=\"month\")\n",
    "    year = L.Input((1,), name=\"year\")\n",
    "    event = L.Input((1,), name=\"event\")\n",
    "    nday = L.Input((1,), name=\"nday\")\n",
    "    item = L.Input((1,), name=\"item\")\n",
    "    dept = L.Input((1,), name=\"dept\")\n",
    "    cat = L.Input((1,), name=\"cat\")\n",
    "    store = L.Input((1,), name=\"store\")\n",
    "    state = L.Input((1,), name=\"state\")\n",
    "    inp = {\"snap_CA\":ca, \"snap_TX\":tx, \"snap_WI\":wi, \"wday\":wday, \n",
    "           \"month\":month, \"year\":year, \"event\":event, \"nday\":nday,\n",
    "           \"item\":item, \"dept\":dept, \"cat\":cat, \"store\":store, \n",
    "           \"state\":state, \"num\":num} \n",
    "    #\n",
    "    ca_ = L.Embedding(mods[\"snap_CA\"], mods[\"snap_CA\"], name=\"ca_3d\")(ca)\n",
    "    tx_ = L.Embedding(mods[\"snap_TX\"], mods[\"snap_TX\"], name=\"tx_3d\")(tx)\n",
    "    wi_ = L.Embedding(mods[\"snap_WI\"], mods[\"snap_WI\"], name=\"wi_3d\")(wi)\n",
    "    wday_ = L.Embedding(mods[\"wday\"], mods[\"wday\"], name=\"wday_3d\")(wday)\n",
    "    month_ = L.Embedding(mods[\"month\"], mods[\"month\"], name=\"month_3d\")(month)\n",
    "    year_ = L.Embedding(mods[\"year\"], mods[\"year\"], name=\"year_3d\")(year)\n",
    "    event_ = L.Embedding(mods[\"event_name\"], mods[\"event_name\"], name=\"event_3d\")(event)\n",
    "    nday_ = L.Embedding(mods[\"nday\"], mods[\"nday\"], name=\"nday_3d\")(nday)\n",
    "    item_ = L.Embedding(mods[\"item_id\"], 10, name=\"item_3d\")(item)\n",
    "    dept_ = L.Embedding(mods[\"dept_id\"], mods[\"dept_id\"], name=\"dept_3d\")(dept)\n",
    "    cat_ = L.Embedding(mods[\"cat_id\"], mods[\"cat_id\"], name=\"cat_3d\")(cat)\n",
    "    store_ = L.Embedding(mods[\"store_id\"], mods[\"store_id\"], name=\"store_3d\")(store)\n",
    "    state_ = L.Embedding(mods[\"state_id\"], mods[\"state_id\"], name=\"state_3d\")(state)\n",
    "    \n",
    "    p = [ca_, tx_, wi_, wday_, month_, year_, event_, nday_, item_, dept_, cat_, store_, state_]\n",
    "    emb = L.Concatenate(name=\"embds\")(p)\n",
    "    context = L.Flatten(name=\"context\")(emb)\n",
    "    \n",
    "    x = L.Concatenate(name=\"x1\")([context, num])\n",
    "    x = L.Dense(500, activation=\"relu\", name=\"d1\")(x)\n",
    "    x = L.Dropout(0.3)(x)\n",
    "    x = L.Concatenate(name=\"m1\")([x, context])\n",
    "    x = L.Dense(500, activation=\"relu\", name=\"d2\")(x)\n",
    "    x = L.Dropout(0.3)(x)\n",
    "    x = L.Concatenate(name=\"m2\")([x, context])\n",
    "    x = L.Dense(500, activation=\"relu\", name=\"d3\")(x)\n",
    "    preds = L.Dense(9, activation=\"linear\", name=\"preds\")(x)\n",
    "    model = M.Model(inp, preds, name=\"M1\")\n",
    "    model.compile(loss=qloss, optimizer=\"adam\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = make_model(len(FEATS))\n",
    "ckpt = ModelCheckpoint(\"w.h5\", monitor='val_loss', verbose=1, save_best_only=True,mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001)\n",
    "es = EarlyStopping(monitor='val_loss', patience=3)\n",
    "print(net.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fit(xt, yt, batch_size=50_000, epochs=20, validation_data=(xv, yv), callbacks=[ckpt, reduce_lr, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nett = make_model(len(FEATS))\n",
    "nett.load_weights(\"w.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv = nett.predict(xv, batch_size=50_000, verbose=1)\n",
    "pe = nett.predict(xe, batch_size=50_000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nett.evaluate(xv, yv, batch_size=50_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv = pv.reshape((-1, 28, 9))\n",
    "pe = pe.reshape((-1, 28, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = sv.reshape((-1, 28))\n",
    "se = se.reshape((-1, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yv = yv.reshape((-1, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.random.randint(0, 42840)\n",
    "#k = np.random.randint(0, 200)\n",
    "print(ids[k, 0])\n",
    "plt.plot(np.arange(28, 56), Yv[k], label=\"true\")\n",
    "plt.plot(np.arange(28, 56), pv[k ,:, 3], label=\"q25\")\n",
    "plt.plot(np.arange(28, 56), pv[k ,:, 4], label=\"q50\")\n",
    "plt.plot(np.arange(28, 56), pv[k, :, 5], label=\"q75\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [f\"F{i+1}\" for i in range(28)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piv = pd.DataFrame(ids[:, 0], columns=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUANTILES = [\"0.005\", \"0.025\", \"0.165\", \"0.250\", \"0.500\", \"0.750\", \"0.835\", \"0.975\", \"0.995\"]\n",
    "VALID = []\n",
    "EVAL = []\n",
    "\n",
    "for i, quantile in tqdm(enumerate(QUANTILES)):\n",
    "    t1 = pd.DataFrame(pv[:,:, i]*sv, columns=names)\n",
    "    t1 = piv.join(t1)\n",
    "    t1[\"id\"] = t1[\"id\"] + f\"_{quantile}_validation\"\n",
    "    t2 = pd.DataFrame(pe[:,:, i]*se, columns=names)\n",
    "    t2 = piv.join(t2)\n",
    "    t2[\"id\"] = t2[\"id\"] + f\"_{quantile}_evaluation\"\n",
    "    VALID.append(t1)\n",
    "    EVAL.append(t2)\n",
    "#============#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub = sub.append(VALID + EVAL)\n",
    "del VALID, EVAL, t1, t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('./submission_from_keras.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
